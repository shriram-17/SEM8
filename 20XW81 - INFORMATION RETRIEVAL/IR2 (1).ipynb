{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKPJNIK7uQ_q",
        "outputId": "abb9d341-39da-4b65-ae68-3a72dce9d401"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-learn\n",
        "!pip install nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "ps = PorterStemmer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvrUymmvukCo",
        "outputId": "39903521-6c73-40d7-d683-6f518e0d8bd5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "database={\"Doc1\": \"The quick brown fox jumps over the lazy dog\",\n",
        "          \"Doc2\": \"The lazy dog likes to sleep all day\" ,\n",
        "           \"Doc3\": \"The brown fox prefers to eat cheese\",\n",
        "            \"Doc4\": \"The red fox jumps over the brown fox\",\n",
        "            \"Doc5\": \"The brown dog chases the fox\"\n",
        "}\n",
        "database"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9ILYeGauuLw",
        "outputId": "59de4b0d-0c21-480e-c8c7-15d6919f3b93"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Doc1': 'The quick brown fox jumps over the lazy dog',\n",
              " 'Doc2': 'The lazy dog likes to sleep all day',\n",
              " 'Doc3': 'The brown fox prefers to eat cheese',\n",
              " 'Doc4': 'The red fox jumps over the brown fox',\n",
              " 'Doc5': 'The brown dog chases the fox'}"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def splitData(database):\n",
        "    transformed_database ={}\n",
        "    for title,content in zip(database.keys(),database.values()):\n",
        "        transformed_database[title] = content.split(\" \")\n",
        "    return transformed_database"
      ],
      "metadata": {
        "id": "1VsfRaUlxvtJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords(database):\n",
        "    sample_database = {}\n",
        "    for title,content in zip(database.keys(),database.values()):\n",
        "          temp = \" \"\n",
        "          content = [i for i in content if i not in [\"a\", \"the\" ,\"an\", \"and\",'had','for','your'] ]\n",
        "          temp = \" \".join(content)\n",
        "          sample_database[title] = temp\n",
        "    return sample_database"
      ],
      "metadata": {
        "id": "88PJbitju5ZZ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_words(database):\n",
        "    modified_database = {}\n",
        "    for title,content in zip(database.keys(),database.values()):\n",
        "        words = word_tokenize(content)\n",
        "        temp = []\n",
        "        for w in words:\n",
        "            temp.append(ps.stem(w))\n",
        "            modified_database[title] = temp\n",
        "    return modified_database"
      ],
      "metadata": {
        "id": "-OFhxGV9vEQh"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "database.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRt0NneK0wpr",
        "outputId": "825922e2-55c5-4ea3-916c-4f15380faa16"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['Doc1', 'Doc2', 'Doc3', 'Doc4', 'Doc5', 'Doc6', 'Doc7'])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def findCount (context,index,indexed_word):\n",
        "  count = 0\n",
        "  for word in context:\n",
        "     if word == indexed_word:\n",
        "        count = count + 1\n",
        "  return count\n",
        "\n",
        "def Termfrenquency(database):\n",
        "    database_with_index= {}\n",
        "    for title,content in zip(database.keys(),database.values()):\n",
        "         sample_lst = []\n",
        "         for word in content:\n",
        "            dict1 = {}\n",
        "            count = findCount(content,title,word)\n",
        "            dict1[word] = count\n",
        "            sample_lst.append(dict1)\n",
        "         if len(sample_lst) == len(content):\n",
        "            database_with_index[title] = sample_lst\n",
        "    return database_with_index\n",
        "\n",
        "def InverseTermDoucmentFrenquecy():\n",
        "    pass"
      ],
      "metadata": {
        "id": "i7clITi4vV5g"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = splitData(database)"
      ],
      "metadata": {
        "id": "p90AQetY0jmj"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = remove_stopwords(data)\n",
        "data = tokenize_words(data)\n",
        "database_with_index =Termfrenquency(data)\n",
        "data"
      ],
      "metadata": {
        "id": "5sN6FM610hSl"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "{\n",
        "  \"Doc_Name\" : List of dictionaries with \"Term\" and its \"Count\"\n",
        "}\n",
        "'''\n",
        "'''\n",
        "Remove the tokeniztion\n",
        "Stop words\n",
        "port stemmer\n",
        "'''"
      ],
      "metadata": {
        "id": "XbC2PuMrAqiK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b210ce6-0fa7-41c0-ac9a-832a0191e809"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(database_with_index))"
      ],
      "metadata": {
        "id": "EGjheQ29AW5G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dd6214a-e6db-4a2d-f3b9-850cd21eb5f4"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7\n"
          ]
        }
      ]
    }
  ]
}